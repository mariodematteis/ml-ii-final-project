@article{EckerliFlorian2021GANi,
  abstract = {Modelling in finance is a challenging task: the data often has complex
  statistical properties and its inner workings are largely unknown. Deep
  learning algorithms are making progress in the field of data-driven modelling,
  but the lack of sufficient data to train these models is currently holding back
  several new applications. Generative Adversarial Networks (GANs) are a neural
  network architecture family that has achieved good results in image generation
  and is being successfully applied to generate time series and other types of
  financial data. The purpose of this study is to present an overview of how
  these GANs work, their capabilities and limitations in the current state of
  research with financial data, and present some practical applications in the
  industry. As a proof of concept, three known GAN architectures were tested on
  financial time series, and the generated data was evaluated on its statistical
  properties, yielding solid results. Finally, it was shown that GANs have made
  considerable progress in their finance applications and can be a solid
  additional tool for data scientists in this field.},
  author = {Eckerli, Florian and Osterrieder, Joerg},
  copyright = {http://creativecommons.org/licenses/by/4.0},
  language = {eng},
  title = {Generative Adversarial Networks in finance: an overview},
  year = {2021},
}

@misc{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {{arXiv}:1406.2661},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2025-02-25},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/6KPVH2RU/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/PAJXEG8U/1406.html:text/html},
}

@article{cont_empirical_2001,
	title = {Empirical properties of asset returns: stylized facts and statistical issues},
	volume = {1},
	issn = {1469-7688},
	url = {https://econpapers.repec.org/article/tafquantf/v_3a1_3ay_3a2001_3ai_3a2_3ap_3a223-236.htm},
	shorttitle = {Empirical properties of asset returns},
	abstract = {We present a set of stylized empirical facts emerging from the statistical analysis of price variations in various types of financial markets. We first discuss some general issues common to all statistical studies of financial time series. Various statistical properties of asset returns are then described: distributional properties, tail properties and extreme fluctuations, pathwise regularity, linear and nonlinear dependence of returns in time and across stocks. Our description emphasizes properties common to a wide variety of markets and instruments. We then show how these statistical properties invalidate many of the common statistical approaches used to study financial data sets and examine some of the statistical problems encountered in each case.},
	pages = {223--236},
	number = {2},
	journaltitle = {Quantitative Finance},
	author = {Cont, R.},
	urldate = {2024-05-29},
	date = {2001},
	note = {Publisher: Taylor \& Francis Journals},
}

@article{iglesias_survey_2023,
	title = {A survey on {GANs} for computer vision: Recent research, analysis and taxonomy},
	volume = {48},
	issn = {15740137},
	url = {http://arxiv.org/abs/2203.11242},
	doi = {10.1016/j.cosrev.2023.100553},
	shorttitle = {A survey on {GANs} for computer vision},
	abstract = {In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks ({GANs}). {GANs} not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that {GANs} have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of {GANs}, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of {GANs} and the frequently used loss functions will be analyzed. The final objective of this survey is to provide a summary of the evolution and performance of the {GANs} which are having better results to guide future researchers in the field.},
	pages = {100553},
	journaltitle = {Computer Science Review},
	shortjournal = {Computer Science Review},
	author = {Iglesias, Guillermo and Talavera, Edgar and Díaz-Álvarez, Alberto},
	urldate = {2025-02-25},
	date = {2023-05},
	eprinttype = {arxiv},
	eprint = {2203.11242 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/27ZEXWP9/Iglesias et al. - 2023 - A survey on GANs for computer vision Recent resea.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/ZUGW8KJS/2203.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-02-25},
	date = {2023-08-02},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/NF2IA3TF/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/UJQU3A86/1706.html:text/html},
}

@misc{qin_dual-stage_2017,
	title = {A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction},
	url = {http://arxiv.org/abs/1704.02971},
	doi = {10.48550/arXiv.1704.02971},
	abstract = {The Nonlinear autoregressive exogenous ({NARX}) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various {NARX} models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network ({DA}-{RNN}) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the {SML} 2010 dataset and the {NASDAQ} 100 Stock dataset demonstrate that the {DA}-{RNN} can outperform state-of-the-art methods for time series prediction.},
	number = {{arXiv}:1704.02971},
	publisher = {{arXiv}},
	author = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison},
	urldate = {2025-02-25},
	date = {2017-08-14},
	eprinttype = {arxiv},
	eprint = {1704.02971 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/GXHPJ4YE/Qin et al. - 2017 - A Dual-Stage Attention-Based Recurrent Neural Netw.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/Q9RIKL99/1704.html:text/html},
}

@article{chen_exploring_2019,
	title = {Exploring the attention mechanism in {LSTM}-based Hong Kong stock price movement prediction},
	rights = {© 2019 Informa {UK} Limited, trading as Taylor \& Francis Group},
	issn = {1469-7688},
	url = {https://www.tandfonline.com/doi/abs/10.1080/14697688.2019.1622287},
	abstract = {State-of-the-art methods using attention mechanism in Recurrent Neural Networks have shown exceptional performance targeting sequential predictions and classifications. We explore the attention mec...},
	journaltitle = {Quantitative Finance},
	author = {Chen, Shun and Ge, Lei},
	urldate = {2025-02-25},
	date = {2019-09-02},
	note = {Publisher: Routledge},
	file = {Snapshot:/Users/Mario/Zotero/storage/I9KZBAE9/14697688.2019.html:text/html},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to Sequence Learning with Neural Networks},
	url = {http://arxiv.org/abs/1409.3215},
	doi = {10.48550/arXiv.1409.3215},
	abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difficult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a fixed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}'14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}'s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difficulty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	number = {{arXiv}:1409.3215},
	publisher = {{arXiv}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	urldate = {2025-02-25},
	date = {2014-12-14},
	eprinttype = {arxiv},
	eprint = {1409.3215 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/97TPKKDS/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/7WDWFB4I/1409.html:text/html},
}

@online{graves_deep_2012,
	title = {Deep learning with long short-term memory networks for financial market predictions - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221717310652},
	author = {Graves, Alex},
	urldate = {2025-02-25},
	date = {2012-01-01},
	file = {Deep learning with long short-term memory networks for financial market predictions - ScienceDirect:/Users/Mario/Zotero/storage/28EDXFMB/S0377221717310652.html:text/html},
}

@misc{bahdanau_neural_2016,
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	number = {{arXiv}:1409.0473},
	publisher = {{arXiv}},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	urldate = {2025-02-25},
	date = {2016-05-19},
	eprinttype = {arxiv},
	eprint = {1409.0473 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/K46IJG46/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/DINU52JC/1409.html:text/html},
}

@article{fischer_deep_2018,
	title = {Deep learning with long short-term memory networks for financial market predictions},
	volume = {270},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221717310652},
	doi = {10.1016/j.ejor.2017.11.054},
	abstract = {Long short-term memory ({LSTM}) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy {LSTM} networks for predicting out-of-sample directional movements for the constituent stocks of the S\&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe ratio of 5.8 prior to transaction costs, we find {LSTM} networks to outperform memory-free classification methods, i.e., a random forest ({RAF}), a deep neural net ({DNN}), and a logistic regression classifier ({LOG}). The outperformance relative to the general market is very clear from 1992 to 2009, but as of 2010, excess returns seem to have been arbitraged away with {LSTM} profitability fluctuating around zero after transaction costs. We further unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected for trading – they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that yields 0.23 percent prior to transaction costs. Further regression analysis unveils low exposure of the {LSTM} returns to common sources of systematic risk – also compared to the three benchmark models.},
	pages = {654--669},
	number = {2},
	journaltitle = {European Journal of Operational Research},
	shortjournal = {European Journal of Operational Research},
	author = {Fischer, Thomas and Krauss, Christopher},
	urldate = {2025-02-25},
	date = {2018-10-16},
	keywords = {Deep learning, Finance, {LSTM}, Machine learning, Statistical arbitrage},
	file = {ScienceDirect Snapshot:/Users/Mario/Zotero/storage/U3TURYWX/S0377221717310652.html:text/html;Submitted Version:/Users/Mario/Zotero/storage/SPFHSKP8/Fischer and Krauss - 2018 - Deep learning with long short-term memory networks.pdf:application/pdf},
}

@article{bao_deep_2017,
	title = {A deep learning framework for financial time series using stacked autoencoders and long-short term memory},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180944},
	doi = {10.1371/journal.pone.0180944},
	abstract = {The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms ({WT}), stacked autoencoders ({SAEs}) and long-short term memory ({LSTM}) are combined for stock price forecasting. The {SAEs} for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by {WT} to eliminate noise. Second, {SAEs} is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into {LSTM} to forecast the next day’s closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.},
	pages = {e0180944},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Bao, Wei and Yue, Jun and Rao, Yulei},
	urldate = {2025-02-25},
	date = {2017-07-14},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Finance, Forecasting, Memory, Neural networks, Recurrent neural networks, Stock markets, Wavelet transforms},
	file = {Full Text PDF:/Users/Mario/Zotero/storage/9NCB4UEB/Bao et al. - 2017 - A deep learning framework for financial time serie.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2025-02-25},
	date = {1997-11-15},
	file = {Snapshot:/Users/Mario/Zotero/storage/FRAKXN34/Long-Short-Term-Memory.html:text/html},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	rights = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2025-02-25},
	date = {2015-05},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
}

@misc{dixon_high_2017,
	title = {A High Frequency Trade Execution Model for Supervised Learning},
	url = {http://arxiv.org/abs/1710.03870},
	doi = {10.48550/arXiv.1710.03870},
	abstract = {This paper introduces a high frequency trade execution model to evaluate the economic impact of supervised machine learners. Extending the concept of a confusion matrix, we present a 'trade information matrix' to attribute the expected profit and loss of the high frequency strategy under execution constraints, such as fill probabilities and position dependent trade rules, to correct and incorrect predictions. We apply the trade execution model and trade information matrix to Level {II} E-mini S\&P 500 futures history and demonstrate an estimation approach for measuring the sensitivity of the P\&L to the error of a Recurrent Neural Network. Our approach directly evaluates the performance sensitivity of a market making strategy to prediction error and augments traditional market simulation based testing.},
	number = {{arXiv}:1710.03870},
	publisher = {{arXiv}},
	author = {Dixon, Matthew F.},
	urldate = {2025-02-25},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1710.03870 [q-fin]},
	keywords = {Quantitative Finance - Trading and Market Microstructure},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/JZEZXFH6/Dixon - 2017 - A High Frequency Trade Execution Model for Supervi.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/ZZU9FNGD/1710.html:text/html},
}

@inproceedings{liu_option_2023,
	title = {Option pricing using deep convolutional neural networks enhanced by technical indicators},
	url = {https://ieeexplore.ieee.org/document/10262865},
	doi = {10.1109/CCIS59572.2023.10262865},
	abstract = {Artificial Neural networks are increasingly employed for option pricing in recent years. However, the pricing ability and effectiveness of convolutional neural networks ({CNNs}) have not been investigated adequately in the field of financial options. Hence, it is interesting to investigate whether convolutional neural networks are effective in pricing Chinese options. In this paper, an innovative idea of combining 2D-{CNN} with market technical indicators ({TIs}) has been implemented. The research is conducted based on the 50ETF options which are obtained from Shanghai Stock Exchange covering a time span from January 2018 to December 2022. Having compared the pricing accuracies between the {CNNs} (with and without {TIs}) and the Black-Scholes model, we show that the method of combining {CNNs} with technical indicators ({TIs}) are effective in pricing Chinese options. This research is useful for practitioners and researchers in the field of option trading.},
	eventtitle = {2023 {IEEE} 9th International Conference on Cloud Computing and Intelligent Systems ({CCIS})},
	pages = {143--147},
	booktitle = {2023 {IEEE} 9th International Conference on Cloud Computing and Intelligent Systems ({CCIS})},
	author = {Liu, David and Wu, Yu},
	urldate = {2025-02-25},
	date = {2023-08},
	note = {{ISSN}: 2376-595X},
	keywords = {Chinese Options, Convolutional Neural Network, Convolutional neural networks, Data models, Deep learning, Option Pricing, Predictive models, Pricing, Technical indicators, Time series analysis, Training},
	file = {IEEE Xplore Abstract Record:/Users/Mario/Zotero/storage/B9F5YQWL/10262865.html:text/html},
}

@article{zhang_deeplob_2019,
	title = {{DeepLOB}: Deep Convolutional Neural Networks for Limit Order Books},
	volume = {67},
	issn = {1053-587X, 1941-0476},
	url = {http://arxiv.org/abs/1808.03668},
	doi = {10.1109/TSP.2019.2907260},
	shorttitle = {{DeepLOB}},
	abstract = {We develop a large-scale deep learning model to predict price movements from limit order book ({LOB}) data of cash equities. The architecture utilises convolutional filters to capture the spatial structure of the limit order books as well as {LSTM} modules to capture longer time dependencies. The proposed network outperforms all existing state-of-the-art algorithms on the benchmark {LOB} dataset [1]. In a more realistic setting, we test our model by using one year market quotes from the London Stock Exchange and the model delivers a remarkably stable out-of-sample prediction accuracy for a variety of instruments. Importantly, our model translates well to instruments which were not part of the training set, indicating the model's ability to extract universal features. In order to better understand these features and to go beyond a "black box" model, we perform a sensitivity analysis to understand the rationale behind the model predictions and reveal the components of {LOBs} that are most relevant. The ability to extract robust features which translate well to other instruments is an important property of our model which has many other applications.},
	pages = {3001--3012},
	number = {11},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	shortjournal = {{IEEE} Trans. Signal Process.},
	author = {Zhang, Zihao and Zohren, Stefan and Roberts, Stephen},
	urldate = {2025-02-25},
	date = {2019-06-01},
	eprinttype = {arxiv},
	eprint = {1808.03668 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance},
	file = {Preprint PDF:/Users/Mario/Zotero/storage/7X2WIIYY/Zhang et al. - 2019 - DeepLOB Deep Convolutional Neural Networks for Li.pdf:application/pdf;Snapshot:/Users/Mario/Zotero/storage/3FNDK7SC/1808.html:text/html},
}
