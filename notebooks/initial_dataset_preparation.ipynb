{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_data_folder: Path = Path(\"../00_raw/\")\n",
    "draft_data_processing_folder: Path = Path(\"../10_draft_processing/\")\n",
    "aggregare_draft_data_folder: Path = Path(\"../20_aggregate_draft/\")\n",
    "prod_data_folder: Path = Path(\"30_prod/\")\n",
    "features_data_folder: Path = Path(\"40_features/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_LIST_FILE: str = \"ticker_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start_date: str = \"2010-01-01\"\n",
    "end_date: str = \"2025-01-01\"\n",
    "expected_weeks: pd.DatetimeIndex = pd.date_range(\n",
    "    start=start_date, end=end_date, freq=\"W\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_count = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_submit(executor, fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Attempts to submit a task to the executor.\n",
    "    If a RuntimeError with \"can't start new thread\" occurs, waits 0.5 seconds and retries.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            return executor.submit(fn, *args, **kwargs)\n",
    "        except RuntimeError as e:\n",
    "            if \"can't start new thread\" in str(e):\n",
    "                print(\n",
    "                    \"Thread creation error encountered. Waiting before retrying submission.\"\n",
    "                )\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker(ticker):\n",
    "    \"\"\"\n",
    "    Downloads weekly data for a ticker from yfinance over the specified date range,\n",
    "    checks if it has data from January 2010 and is not missing a majority of weeks.\n",
    "\n",
    "    Returns a tuple (ticker, series, status) where:\n",
    "      - series is a new pd.Series with the weekly Close prices (aligned to expected_weeks)\n",
    "        if the ticker is acceptable; otherwise None.\n",
    "      - status is \"good\" if accepted or a string explaining the rejection.\n",
    "    \"\"\"\n",
    "    # (Optional) Sleep a little to help stagger downloads.\n",
    "    time.sleep(random.uniform(0.05, 0.2))\n",
    "\n",
    "    # Use a lock to ensure only one thread calls yf.download at a time.\n",
    "    with download_lock:\n",
    "        try:\n",
    "            data = yf.download(\n",
    "                ticker,\n",
    "                start=START_DATE,\n",
    "                end=END_DATE,\n",
    "                interval=\"1wk\",\n",
    "                progress=False,\n",
    "                auto_adjust=True,\n",
    "                threads=False,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return (ticker, None, f\"download_error: {e}\")\n",
    "\n",
    "    if data.empty:\n",
    "        return (ticker, None, \"no_data\")\n",
    "\n",
    "    first_date = data.index.min().to_pydatetime().date()\n",
    "    if first_date > datetime.strptime(START_DATE, \"%Y-%m-%d\").date():\n",
    "        return (ticker, None, \"insufficient_history\")\n",
    "\n",
    "    if len(data) < 0.5 * len(expected_weeks):\n",
    "        return (ticker, None, \"insufficient_data_points\")\n",
    "\n",
    "    # Extract the \"Close\" column.\n",
    "    series = data[\"Close\"]\n",
    "    if isinstance(series, pd.DataFrame):\n",
    "        series = series.iloc[:, 0]\n",
    "\n",
    "    # Reindex the series to expected weeks using forward fill.\n",
    "    series_aligned = series.reindex(expected_weeks, method=\"ffill\")\n",
    "    # Create a new Series so that it is independent.\n",
    "    series_new = pd.Series(\n",
    "        data=series_aligned.values, index=series_aligned.index, name=ticker\n",
    "    )\n",
    "    return (ticker, series_new, \"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_previous_state():\n",
    "    \"\"\"\n",
    "    Loads the combined backup data along with successful and unsuccessful ticker lists.\n",
    "    Returns:\n",
    "      combined_df: DataFrame with saved ticker data (indexed by expected_weeks).\n",
    "      good_ticker_data: dict mapping ticker to its pd.Series.\n",
    "      processed_tickers: dict mapping ticker to its status (\"good\" or error string).\n",
    "      unsuccessful_set: set of tickers known to be unsuccessful.\n",
    "      saved_count: number of tickers already saved (from backup files).\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame(index=expected_weeks)\n",
    "    good_ticker_data = {}\n",
    "    processed_tickers = {}\n",
    "\n",
    "    # Load successful tickers.\n",
    "    try:\n",
    "        success_df = pd.read_csv(SUCCESSFUL_TICKERS_FILE)\n",
    "        successful_tickers = success_df[\"Ticker\"].dropna().tolist()\n",
    "    except Exception:\n",
    "        successful_tickers = []\n",
    "    # Load unsuccessful tickers.\n",
    "    try:\n",
    "        unsuccess_df = pd.read_csv(UNSUCCESSFUL_TICKERS_FILE)\n",
    "        unsuccessful_tickers = unsuccess_df[\"Ticker\"].dropna().tolist()\n",
    "    except Exception:\n",
    "        unsuccessful_tickers = []\n",
    "\n",
    "    for ticker in successful_tickers:\n",
    "        processed_tickers[ticker] = \"good\"\n",
    "    for ticker in unsuccessful_tickers:\n",
    "        processed_tickers[ticker] = \"failed\"\n",
    "\n",
    "    # Create a set for quick lookup of unsuccessful tickers.\n",
    "    unsuccessful_set = set(unsuccessful_tickers)\n",
    "\n",
    "    # Check for backup files. We assume backups are named like \"ticker_data_backup_{n}.csv\"\n",
    "    backup_files = glob.glob(\n",
    "        os.path.join(DATA_BACKUP_FOLDER, \"ticker_data_backup_*.csv\")\n",
    "    )\n",
    "    saved_count = 0\n",
    "    if backup_files:\n",
    "\n",
    "        def extract_count(fn):\n",
    "            base = os.path.basename(fn)\n",
    "            try:\n",
    "                return int(base.replace(\"ticker_data_backup_\", \"\").replace(\".csv\", \"\"))\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        backup_files.sort(key=extract_count)\n",
    "        latest_backup = backup_files[-1]\n",
    "        try:\n",
    "            combined_df = pd.read_csv(latest_backup, index_col=0, parse_dates=True)\n",
    "            for ticker in combined_df.columns:\n",
    "                good_ticker_data[ticker] = combined_df[ticker]\n",
    "            saved_count = len(combined_df.columns)\n",
    "            print(f\"Loaded backup from {latest_backup} with {saved_count} tickers.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading backup file {latest_backup}: {e}\")\n",
    "    else:\n",
    "        print(\"No previous backup file found.\")\n",
    "\n",
    "    return (\n",
    "        combined_df,\n",
    "        good_ticker_data,\n",
    "        processed_tickers,\n",
    "        unsuccessful_set,\n",
    "        saved_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_df = pd.read_csv(TICKER_LIST_FILE)\n",
    "all_tickers = tickers_df[\"Ticker\"].dropna().unique().tolist()\n",
    "\n",
    "# Load previously saved state.\n",
    "(combined_df, good_ticker_data, processed_tickers, unsuccessful_set, saved_count) = (\n",
    "    load_previous_state()\n",
    ")\n",
    "print(\n",
    "    f\"Starting with {len(good_ticker_data)} good tickers and {len(processed_tickers)} total processed tickers.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            while len(good_ticker_data) < TARGET_COUNT and len(processed_tickers) < len(all_tickers):\n",
    "                # Only consider tickers not already processed.\n",
    "                remaining = list(set(all_tickers) - set(processed_tickers.keys()))\n",
    "                if not remaining:\n",
    "                    print(\"No more tickers remaining to sample.\")\n",
    "                    break\n",
    "\n",
    "                batch_size = min(20, len(remaining))\n",
    "                batch = random.sample(remaining, batch_size)\n",
    "                future_to_ticker = {}\n",
    "\n",
    "                # Mark tickers as scheduled immediately to avoid duplicates in future batches.\n",
    "                for ticker in batch:\n",
    "                    processed_tickers[ticker] = \"scheduled\"\n",
    "                    future = safe_submit(executor, process_ticker, ticker)\n",
    "                    future_to_ticker[future] = ticker\n",
    "\n",
    "                for future in as_completed(future_to_ticker):\n",
    "                    ticker = future_to_ticker[future]\n",
    "                    try:\n",
    "                        ticker_symbol, series, status = future.result()\n",
    "                    except Exception as exc:\n",
    "                        processed_tickers[ticker] = f\"exception: {exc}\"\n",
    "                        if ticker not in unsuccessful_set:\n",
    "                            pd.DataFrame([[ticker, f\"exception: {exc}\"]],\n",
    "                                         columns=[\"Ticker\", \"Status\"]) \\\n",
    "                              .to_csv(UNSUCCESSFUL_TICKERS_FILE, mode=\"a\", header=False, index=False)\n",
    "                            unsuccessful_set.add(ticker)\n",
    "                        continue\n",
    "\n",
    "                    processed_tickers[ticker_symbol] = status\n",
    "\n",
    "                    if status == \"good\" and series is not None:\n",
    "                        combined_df[ticker_symbol] = series.copy()\n",
    "                        good_ticker_data[ticker_symbol] = series.copy()\n",
    "                        print(f\"Ticker {ticker_symbol} accepted. Total good tickers: {len(good_ticker_data)}\")\n",
    "                        pd.DataFrame([[ticker_symbol]], columns=[\"Ticker\"]).to_csv(\n",
    "                            SUCCESSFUL_TICKERS_FILE, mode=\"a\", header=False, index=False)\n",
    "                    else:\n",
    "                        if ticker_symbol not in unsuccessful_set:\n",
    "                            pd.DataFrame([[ticker_symbol, status]], columns=[\"Ticker\", \"Status\"]) \\\n",
    "                              .to_csv(UNSUCCESSFUL_TICKERS_FILE, mode=\"a\", header=False, index=False)\n",
    "                            unsuccessful_set.add(ticker_symbol)\n",
    "\n",
    "                # Short pause between batches to allow thread cleanup.\n",
    "                time.sleep(0.1)\n",
    "\n",
    "                if (len(good_ticker_data) - saved_count) >= BACKUP_INTERVAL:\n",
    "                    backup_csv = os.path.join(DATA_BACKUP_FOLDER, f\"ticker_data_backup_{len(good_ticker_data)}.csv\")\n",
    "                    combined_df.to_csv(backup_csv)\n",
    "                    print(f\"Backup saved after reaching {len(good_ticker_data)} good tickers.\")\n",
    "                    saved_count = len(good_ticker_data)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interrupt received. Shutting down threads gracefully.\")\n",
    "    finally:\n",
    "        final_data_csv = os.path.join(DATA_BACKUP_FOLDER, \"ticker_data_final.csv\")\n",
    "        final_processed_csv = os.path.join(TICKER_LIST_FOLDER, \"processed_tickers_final.csv\")\n",
    "        combined_df.to_csv(final_data_csv)\n",
    "        pd.DataFrame(list(processed_tickers.items()), columns=[\"Ticker\", \"Status\"]).to_csv(final_processed_csv, index=False)\n",
    "        print(f\"Process complete. {len(good_ticker_data)} good tickers saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SyntheticTimeSeries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
